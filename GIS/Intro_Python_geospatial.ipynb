{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/su-sumico/seminar/blob/main/Intro_Python_geospatial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "S6xi1qinKvA_"
      },
      "cell_type": "markdown",
      "source": [
        "## Session Prep"
      ]
    },
    {
      "metadata": {
        "id": "gSddwN1KC74U"
      },
      "cell_type": "markdown",
      "source": [
        "1. Run the cell below to install two libraries we'll be using to look at geospatial data: GeoPandas and OpticalRS.\n",
        "2. Visit the [Github repo](https://github.com/rrcarlson/Intro_python_geo) and download/clone the files in the Data folder (shapefiles and raster)."
      ]
    },
    {
      "metadata": {
        "id": "zP7vdH0AK0W3"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Install OpticalRS. This also installs GeoPandas because GeoPandas is a dependency of OpticalRS\n",
        "!apt-get install software-properties-common python-software-properties > /dev/null\n",
        "!add-apt-repository ppa:ubuntugis/ppa -y > /dev/null\n",
        "!apt-get update > /dev/null\n",
        "!apt-get install -y --fix-missing python-gdal gdal-bin libgdal-dev > /dev/null\n",
        "!pip2 install OpticalRS > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fl40H7od1mK4"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro to Python"
      ]
    },
    {
      "metadata": {
        "id": "cGa0avMm1mK5"
      },
      "cell_type": "markdown",
      "source": [
        "![Python image](https://github.com/rrcarlson/Intro_python_geo/blob/master/images/python.jpg?raw=1)\n",
        "\n",
        "Python is a rich, versatile language with lots of prolific contributors. So where to start?\n",
        "\n",
        "For reference, below are some Python tutorials that cover a range of Python libraries. We won't spend too much time today on basic syntax, etc., since that's well covered elsewhere and kinda boring. As a Python newbie, I've found that using Pandas/GeoPandas and NumPy have introduced me organically to other libraries, and have allowed me to see results quickly for basic geospatial tasks--so we'll start there!\n",
        "* [Python basics](https://github.com/sinkovit/PythonSeries/blob/master/Python%20basics.ipynb): How to create lists, dictionaries, iterators, etc.\n",
        "* [Python Markdown and LaTex](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet): Documentation and formatting\n",
        "* Matplotlib: Data visualization\n",
        "* [Pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html): Data wrangling and preparation using two types of data objects: 1) DataFrames, 2) Series.\n",
        "* [GeoPandas](https://automating-gis-processes.github.io/2016/Lesson2-overview-pandas-geopandas.html): Basically the geospatial version of Pandas. Adds geospatial functionality to the DataFrame (Geo + DataFrame = GeoDataFrame).\n",
        "* [NumPy](https://www.datacamp.com/community/tutorials/python-numpy-tutorial): Multi-dimensional data arrays for manipulating large datasets. There are some functions for linear algebra and statistics within NumPy but it's also used by SciPy, matplotlib, and pandas for scientific computing.\n",
        "* [SciPy](http://www.randalolson.com/2012/08/06/statistical-analysis-made-easy-in-python/): Scientific computing and statistical analysis. Uses NumPy, Matplotlib, and Pandas, and is common in earth sciences, astronomy, and oceanography.\n",
        "* [Shapely](http://toblerity.org/shapely/manual.html): Spatial data model for planar features (points, curves, and surfaces).\n",
        "* Skikit-learn: regression, clustering, and classification algorithms\n",
        "* Various [cool tricks](https://community.modeanalytics.com/python/)"
      ]
    },
    {
      "metadata": {
        "id": "kH0cCmSd1mK5"
      },
      "cell_type": "markdown",
      "source": [
        "# Conservation in the Dominican Republic"
      ]
    },
    {
      "metadata": {
        "id": "vTbdGoGL1mK6"
      },
      "cell_type": "markdown",
      "source": [
        "![Reef image](https://github.com/rrcarlson/Intro_python_geo/blob/master/images/01_coral_reef_bright_spot.jpg?raw=1)\n",
        "\n",
        "In this tutorial, we'll be testing out two Python libraries, **GeoPandas** and **NumPy**, that are useful in GIS. Our objectives: to filter and analyze vector and raster data for conservation areas in the Dominican Republic.\n",
        "\n",
        "We'll use two datasets:\n",
        "* **Conservation Areas of the Dominican Republic**: Vector data delineating conservation area boundaries, from the [World Database of Protected Areas (WDPA)](https://www.iucn.org/theme/protected-areas/our-work/world-database-protected-areas).\n",
        "* **Gridded Bathymetric Data**: Raster data (30 arc-second resolution) on bathymetry from the [British Oceanographic Data Center](https://www.bodc.ac.uk/).\n",
        "\n",
        "Our goals are:\n",
        "* **Filter the WDPA database** to find only conservation areas for the marine environment (Marine Protected Areas)\n",
        "* **Clip rasters** to MPA boundaries\n",
        "* **Analyze raster data** (bathymetry) within Marine Protected Area boundaries to find\n",
        "    * Mean depth per MPA\n",
        "    * Standard deviation of depth per MPA\n",
        "    * Range of depth per MPA"
      ]
    },
    {
      "metadata": {
        "id": "OG90pHk51mK6"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 1: Load your libraries"
      ]
    },
    {
      "metadata": {
        "id": "uU77Mqop1mK7"
      },
      "cell_type": "markdown",
      "source": [
        "What modules do we need?\n",
        "* Vector data comes in a tabular format, with one geometric feature per row. So we need a library to organize geographic data in a DataFrame.\n",
        "**That's Pandas** (more specifically, **GeoPandas**).\n",
        "* Raster data is a little trickier. It comes to us in a grid of pixel values, sometimes in multiple bands (3+ dimensions). We need a library to organize arrays/matrices of values in *n* dimensions. **That's NumPy**.\n",
        "* Numpy isn't explicitly designed to handle raster data. We need the help of another library to convert our raster files into array format. **That's [OpticalRS](https://github.com/jkibele/OpticalRS)**, created by NCEAS' very own Jared Kibele! OpticalRS borrows a bit of code from [an old version of rasterstats](https://github.com/perrygeo/python-rasterstats/releases/tag/0.5) to do the actual raster subsetting.\n",
        "* We'll want to plot stuff, so we need a library for data visualization. That's **matplotlib**.\n",
        "\n",
        "![arrays image](https://github.com/rrcarlson/Intro_python_geo/blob/master/images/arrays.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "20rpNBAb1mK8"
      },
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from OpticalRS import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8J29u-kf1mK_"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 2: Prep your vector data"
      ]
    },
    {
      "metadata": {
        "id": "nJRFiGhI1mK_"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2.1 Read in your vector data: GeoPandas"
      ]
    },
    {
      "metadata": {
        "id": "nyCloPks1mLA"
      },
      "cell_type": "markdown",
      "source": [
        "Reading in tabular data is pretty simple. Just remember:\n",
        "* DataFrame = Pandas = `pd.read_csv()`\n",
        "* GeoDataFrame = DataFrame with geometry attribute = GeoPandas = `gpd.read_file()`"
      ]
    },
    {
      "metadata": {
        "id": "ytyKh93J4cR3"
      },
      "cell_type": "code",
      "source": [
        "# Loading my local files into Colaboratory\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6lZggwYl1mLB"
      },
      "cell_type": "code",
      "source": [
        "# Define your vector filepath\n",
        "conserve_fp = \"WDPA_polygons.shp\"\n",
        "\n",
        "# Read vector data in as GeoDataFrame\n",
        "conserve = gpd.read_file(conserve_fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qiWCmE0T1mLD"
      },
      "cell_type": "markdown",
      "source": [
        "Our first preprocessing step is to check on the data's projection. Projections can be tricky and I'm not going to delve too far into this topic here, but below are some basic steps for identifying and changing your data's projection (more details can be found [here](http://geopandas.org/projections.html))."
      ]
    },
    {
      "metadata": {
        "id": "D3IQ6kAJ1mLD"
      },
      "cell_type": "code",
      "source": [
        "# Check the Coordinate Reference System (CRS)\n",
        "conserve.crs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j8m5oyP-1mLL"
      },
      "cell_type": "code",
      "source": [
        "# Change your CRS, for fun. Then change it back (because we like EPSG = 4326).\n",
        "conserve = conserve.to_crs(epsg=3857)\n",
        "conserve = conserve.to_crs(epsg=4326)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F82jApzI1mLO"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.1"
      ]
    },
    {
      "metadata": {
        "id": "4luBoD9C1mLO"
      },
      "cell_type": "code",
      "source": [
        "# Read in your WDPA shapefile as a GeoDataFrame.\n",
        "\n",
        "# Check the projection of your data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fD5yzPop1mLQ"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2.2 Explore your vector data: GeoPandas"
      ]
    },
    {
      "metadata": {
        "id": "sn6Xos8T1mLS"
      },
      "cell_type": "markdown",
      "source": [
        "We can use some basic exploratory functions in GeoPandas to query our data."
      ]
    },
    {
      "metadata": {
        "id": "WKdLg7jhjAbU"
      },
      "cell_type": "code",
      "source": [
        "# Look at the first few features in our attribute table\n",
        "conserve.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zeCs2GRG1mLT"
      },
      "cell_type": "code",
      "source": [
        "# Look at all of the attributes in your dataset\n",
        "conserve.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tkiCTJZf1mLX"
      },
      "cell_type": "markdown",
      "source": [
        "There are 143 data objects with 29 attributes, of various data types (object, float, integer). In the pandas world, data types break down like this:\n",
        "* dtype('O') = object = can include string and other geopandas object types\n",
        "* dtype('int64') = integer\n",
        "* dtype('float64') = float"
      ]
    },
    {
      "metadata": {
        "id": "u3qTTUvU1mLY"
      },
      "cell_type": "code",
      "source": [
        "# What is the total conserved area in the DR? Conserved area = \"GIS_AREA\" (we'll discuss how to use the inherent geometry of the GeoDataFrame to do this later).\n",
        "area_sum = conserve.GIS_AREA.sum()\n",
        "area_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNZyFN0X1mLc"
      },
      "cell_type": "code",
      "source": [
        "# What is the largest area conserved?\n",
        "area_max = conserve.GIS_AREA.max()\n",
        "area_max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EhTlDpWb1mLf"
      },
      "cell_type": "code",
      "source": [
        "# What is the smallest area conserved?\n",
        "area_min = conserve.GIS_AREA.min()\n",
        "area_min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eWbK-61m1mLj"
      },
      "cell_type": "code",
      "source": [
        "# What are the unique values for the \"MARINE\" attribute?\n",
        "conserve['MARINE'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqKSPaqu1mLn"
      },
      "cell_type": "code",
      "source": [
        "# How many different areas are \"Marine\" (2), \"Terrestrial\" (0), or both (1)?\n",
        "type_park = conserve['MARINE'].value_counts()\n",
        "type_park"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DzWE-9Ky1mLq"
      },
      "cell_type": "code",
      "source": [
        "# Write all of this info into a string\n",
        "\"The total conserved area is {} square meters and there are {} MPAs, {} terrestrial areas, and {} mixed areas\".format(area_sum, type_park[\"2\"], type_park[\"0\"], type_park[\"1\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9zV-u_cC1mLs"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.2"
      ]
    },
    {
      "metadata": {
        "id": "EyfTJmGV1mLt"
      },
      "cell_type": "code",
      "source": [
        "# What are the options for park designation in English (\"DESIG_ENG\")?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sK7E6rjZ1mLv"
      },
      "cell_type": "code",
      "source": [
        "# How many different areas are designated as \"Wildlife Refuge\"?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o2QqMb171mLz"
      },
      "cell_type": "markdown",
      "source": [
        "For a complete list of methods available for exploring GeoDataFrames, see [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)."
      ]
    },
    {
      "metadata": {
        "id": "qjl7rkUL1mL0"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2.3 Plot your vector data: GeoPandas"
      ]
    },
    {
      "metadata": {
        "id": "-4UoYzZn1mL0"
      },
      "cell_type": "markdown",
      "source": [
        "GeoPandas offers some basic plotting methods for quick visualization. You can create more advanced plots (with customizable layering, formatting) using the Matplotlib library. Matplotlib can be a bit confusing at first, so here's a [great tutorial](https://github.com/matplotlib/AnatomyOfMatplotlib/blob/master/AnatomyOfMatplotlib-Part1-Figures_Subplots_and_layouts.ipynb) if you want to learn more. As in R, you can also create interactive maps through [Leaflet](https://automating-gis-processes.github.io/2016/Lesson5-interactive-map-folium.html) and [Bokeh](https://automating-gis-processes.github.io/2016/Lesson5-interactive-map-bokeh.html)."
      ]
    },
    {
      "metadata": {
        "id": "gSHZDyO81mL1"
      },
      "cell_type": "code",
      "source": [
        "# Plot conservation areas\n",
        "conserve.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQGfTSlV1mL3"
      },
      "cell_type": "code",
      "source": [
        "# Plot conservation areas as choropleth by park type\n",
        "conserve.plot(column = \"DESIG_ENG\", cmap = \"Paired\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6nX3UmE1mL5"
      },
      "cell_type": "code",
      "source": [
        "# Make it red! Color palettes: https://matplotlib.org/users/colormaps.html\n",
        "conserve.plot(column = \"DESIG_ENG\", cmap = \"hot\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itP1yj0r1mL7"
      },
      "cell_type": "code",
      "source": [
        "# Plot conservation areas where the area has a Use Management Plan.\n",
        "conserve[conserve['MANG_PLAN'] != \"No\"].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wz7HOiZP1mL_"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.3"
      ]
    },
    {
      "metadata": {
        "id": "9g8pH6vZ1mL_"
      },
      "cell_type": "code",
      "source": [
        "# Plot conservation areas as a choropleth by the \"MARINE\" attribute\n",
        "conserve.plot(column = \"MARINE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_HjXmxEf1mMB"
      },
      "cell_type": "code",
      "source": [
        "# Make it blue (obviously)! https://matplotlib.org/users/colormaps.html\n",
        "conserve.plot(column = 'MARINE', cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DVjnH-z71mMD"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2.4 Filter your vector data: GeoPandas"
      ]
    },
    {
      "metadata": {
        "id": "sToRQcBk1mME"
      },
      "cell_type": "markdown",
      "source": [
        "**Looking above at our \"To Do\" list, our first task was this:** \"Filter the WDPA database to find only conservation areas for the marine environment (Marine Protected Areas).\"\n",
        "\n",
        "In R-speak, we're using Python to `dplyr::filter`. To subset data in Python, we use the `loc` and `iloc` indexing tools. Documentation can be found [here](http://pandas.pydata.org/pandas-docs/version/0.17/indexing.html) under \"Indexing and Selecting Data\" (basically the Python version of R's Data Wrangling cheat sheet).\n",
        "\n",
        "The basics are this:\n",
        "* `iloc` selects data on a known index (integer value). Example: `iris.iloc[7]` would select the 8th row of data.\n",
        "* `loc` selects data on a known label. Example `iris.loc['sepal_length'>7]` selects data with sepal length over 7 centimeters.\n",
        "\n",
        "Let's try it out:"
      ]
    },
    {
      "metadata": {
        "id": "RAWvIK6O1mME"
      },
      "cell_type": "code",
      "source": [
        "# Take a look at our first few lines of data\n",
        "conserve.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U2j4WUyq1mMH"
      },
      "cell_type": "code",
      "source": [
        "# Select rows 0 through 2, column 4\n",
        "conserve.iloc[0:3, 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "11muwsJe1mMJ"
      },
      "cell_type": "code",
      "source": [
        "# Select index value 3 THROUGH 6 for name and metadata ID\n",
        "conserve.loc[3:6, ['NAME','METADATAID']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Say35Et1mMK"
      },
      "cell_type": "code",
      "source": [
        "# Select indices 3 AND 6 for name and metadata ID\n",
        "conserve.loc[[3,6], ['NAME','METADATAID']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2UxUi_fA1mMN"
      },
      "cell_type": "code",
      "source": [
        "# Filter DataFrame for only those areas with the name \"Jaragua\"\n",
        "conserve.loc[conserve['NAME'] == \"Jaragua\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjw_VO0H1mMO"
      },
      "cell_type": "markdown",
      "source": [
        "The WDPA Data Dictionary that Marine Protected Areas are coded by the ordinal `MARINE` attribute, defined as follows:\n",
        "* 0 = 100% terrestrial PA\n",
        "* 1 = Coastal: marine and terrestrial PA\n",
        "* 2 = 100 % marine PA\n",
        "\n",
        "Let's subset our data by *only* those rows that represent 100% MPAs."
      ]
    },
    {
      "metadata": {
        "id": "gRHgeUtc1mMP"
      },
      "cell_type": "code",
      "source": [
        "# Select only 100% marine protected areas\n",
        "mpa = conserve.loc[conserve['MARINE'] == '2']\n",
        "len(mpa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0IduLWU-1mMR"
      },
      "cell_type": "code",
      "source": [
        "mpa[['MARINE', 'NAME']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5b6roE2-1mMT"
      },
      "cell_type": "markdown",
      "source": [
        "Our resulting DataFrame contains vector data for 7 MPAs in the Dominican Republic."
      ]
    },
    {
      "metadata": {
        "id": "WsT9hcld1mMT"
      },
      "cell_type": "markdown",
      "source": [
        "## Before we move on..."
      ]
    },
    {
      "metadata": {
        "id": "5ioV6gzT1mMT"
      },
      "cell_type": "markdown",
      "source": [
        "**Can you \"pipe\" in Python?** Yes and no. You won't see any \"%>%\" business in Python, but Python syntax allows you to string together multiple operations. The Python mantra is that \"everything is an object\", meaning everything (strings, dataframes, lists, functions, even modules) can be assigned to a variable or passed as an argument to a function.\n",
        "\n",
        "Confusing? Take a look at our code above. We're running multiple operations through our \"conserve\" geodataframe, like: 1) reprojecting it, 2) filtering it, and 3) plotting it. In R, \"piping\" might look something like:"
      ]
    },
    {
      "metadata": {
        "id": "aw07fuqg1mMU"
      },
      "cell_type": "markdown",
      "source": [
        "`conserve %>%`\n",
        "> `st_transform(4326) %>%`\n",
        "   \n",
        "> `dplyr::filter(MARINE == \"2\") %>%`\n",
        "   \n",
        "> `dplyr::select(MARINE, NAME, geometry) %>%`\n",
        "\n",
        "> `plot()`\n",
        "\n",
        "    \n",
        "In Python, \"piping\" works out like this:\n",
        "\n",
        "\n",
        "`conserve.to_crs(epsg=4326).loc[conserve['MARINE']=='2'][['MARINE','NAME','geometry']].plot()`\n",
        "\n",
        "\n",
        "Since \"everything is an object\" in Python, the result of each operation can be worked on by subsequent methods. You can see that this line of code simply combines the functions we used in Step 2.2. Let's see if it works."
      ]
    },
    {
      "metadata": {
        "id": "KruGAyTGwEGj"
      },
      "cell_type": "code",
      "source": [
        "conserve.to_crs(epsg=4326).loc[conserve['MARINE'] == '2'][['MARINE','GIS_AREA','geometry']].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xtKTG_Jd1mMZ"
      },
      "cell_type": "markdown",
      "source": [
        "Woohoo!"
      ]
    },
    {
      "metadata": {
        "id": "wOsgN1Ju1mMa"
      },
      "cell_type": "markdown",
      "source": [
        "## Also before we move on..."
      ]
    },
    {
      "metadata": {
        "id": "Bc1bG_f61mMa"
      },
      "cell_type": "markdown",
      "source": [
        "**You can also perform call kinds of operations using the 'geometry' column (every GeoDataFrame has one).**\n",
        "Above, we used the dataset's \"GIS_AREA\" attribute to run summary statistics. This is because the WDPA dataset uses its own methodology and decision factors for calculating polygon areas. Most datasets won't be this finicky.\n",
        "\n",
        "The \"geometry\" column contains a Shapely geometry object (point, polygon, multi-point, multi-polygon, etc.) and is usually automatically detected when GeoPandas reads in your shapefile. But be careful! The geometry column won't always be called \"geometry\". Use the \"name\" method to double check. If GeoPandas chooses the wrong column for geometry, use the `set_geometry` method to specify which column to use."
      ]
    },
    {
      "metadata": {
        "id": "OLmolzi21mMa"
      },
      "cell_type": "code",
      "source": [
        "# Call your geometry attribute and find its Shapely geometry type\n",
        "conserve.geometry.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ulFxqi91mMf"
      },
      "cell_type": "code",
      "source": [
        "# Check which column GeoPandas has chosen as the \"geometry\" attribute\n",
        "conserve.geometry.name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cpqmpSog1mMh"
      },
      "cell_type": "code",
      "source": [
        "# Change if necessary (not necessary in our case, so this code is redundant)\n",
        "conserve.set_geometry(\"geometry\")\n",
        "conserve.geometry.name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxfEIGLS1mMl"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 3: Prep your raster data"
      ]
    },
    {
      "metadata": {
        "id": "y3_7oebp1mMl"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3.1 Read in your raster data: Numpy and OpticalRS"
      ]
    },
    {
      "metadata": {
        "id": "R2G2-U-T1mMm"
      },
      "cell_type": "markdown",
      "source": [
        "**Let's turn our attention to our raster data.** One of my favorite parts of Python is its ability to manipulate raster data. By turning a raster into an object called a NumPy array, you can directly access the matrix of values within raster pixels, analyzing and filtering raster data in [all kinds of ways](https://docs.scipy.org/doc/numpy/user/quickstart.html) within the expansive SciPy library. Arrays are also compatible with GeoPandas, so you don't have to worry about converting your vector data into SpatialPolygonsDataFrames (or whatever) to play well with rasters.\n",
        "\n",
        "To turn our raster file into a numpy array, we'll use a library called OpticalRS by Jared Kibele found [here](https://github.com/jkibele/OpticalRS)."
      ]
    },
    {
      "metadata": {
        "id": "rjTmVRzm1mMm"
      },
      "cell_type": "code",
      "source": [
        "%pylab inline\n",
        "from OpticalRS import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dlqhKmME1mMo"
      },
      "cell_type": "markdown",
      "source": [
        "Our first step is to read our raster file into OpticalRS. The `RasterDS()` function reads in a file and converts it into a raster-specific object called a 'Raster Dataset'. The `band_array` attribute of the 'Raster Dataset' is a NumPy array of your data."
      ]
    },
    {
      "metadata": {
        "id": "cWwbZMbFWV4y"
      },
      "cell_type": "code",
      "source": [
        "# Loading my local files into Colaboratory\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQ65aL9w1mMp"
      },
      "cell_type": "code",
      "source": [
        "# Define your GeoTiff filepath\n",
        "bathy_dr_fp = \"bathy_caribb_4326.tif\"\n",
        "\n",
        "# Turn your GeoTiff into a \"Raster Dataset\" object.\n",
        "rds = RasterDS(bathy_dr_fp)\n",
        "\n",
        "# Convert the \"Raster Dataset\" object into a numpy array\n",
        "dominican_arr = rds.band_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y4svnLWkB9mN"
      },
      "cell_type": "markdown",
      "source": [
        "If you have lots of rasters to read in at once, define your root directory and run a loop through it\n",
        "```\n",
        "in_data_dir = \"/home/carlson/tutorial_files/\"\n",
        "filenames = []\n",
        "for p in os.listdir(in_data_dir):\n",
        "    if p.endswith('.tif'):\n",
        "        fp = os.path.join(in_data_dir, p)\n",
        "        filenames.append(fp)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "xEWJnqQ01mMq"
      },
      "cell_type": "markdown",
      "source": [
        "Let's check out the properties of our array and make sure the conversion worked"
      ]
    },
    {
      "metadata": {
        "id": "oCdZ-e3s1mMs"
      },
      "cell_type": "code",
      "source": [
        "# Check array shape\n",
        "dominican_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q7qjBg3J1mMw"
      },
      "cell_type": "code",
      "source": [
        "# Find mean depth (pixel value) in your raster\n",
        "dominican_arr.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PwPeoMPd1mMy"
      },
      "cell_type": "code",
      "source": [
        "# Check size of array\n",
        "dominican_arr.nbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pxNxhMMk1mM0"
      },
      "cell_type": "code",
      "source": [
        "# Create a quick plot\n",
        "imshow(dominican_arr.squeeze(), cmap ='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bf8ph22Q1mM6"
      },
      "cell_type": "markdown",
      "source": [
        "Looks good!"
      ]
    },
    {
      "metadata": {
        "id": "sASK8U2F1mM6"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.1"
      ]
    },
    {
      "metadata": {
        "id": "8YDqXNVC1mM7"
      },
      "cell_type": "code",
      "source": [
        "# Follow the steps to load in your raster data as a NumPy array."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F5crs_mS1mM8"
      },
      "cell_type": "code",
      "source": [
        "# Find the max bathymetry value in your array (use NumPy documentation: https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.statistics.html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jz6X2zRk1mM9"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 4: Analyze bathymetry by MPA (\"Zonal Statistics\")"
      ]
    },
    {
      "metadata": {
        "id": "ZNBgWire1mM9"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4.1 Apply bathymetry to one \"zone\""
      ]
    },
    {
      "metadata": {
        "id": "O46Q6P901mM-"
      },
      "cell_type": "markdown",
      "source": [
        "Running statistics on our array/raster is easy, but not terribly meaningful because my bounding box is pretty arbitrary. We need to clip our raster to MPAs in order to assess the bathymetry values within our \"zones\" of interest.  This is called \"zonal statistics\" (aka \"cookie cutter statistics\")."
      ]
    },
    {
      "metadata": {
        "id": "KcEsFu561mM-"
      },
      "cell_type": "markdown",
      "source": [
        "![Dwayne the rock image](https://github.com/rrcarlson/Intro_python_geo/blob/master/images/the_rock.png?raw=1)\n",
        "\n",
        "Let's start by clipping our \"Raster Dataset\" object to MPA geometries. This is easy in OpticalRS using the `geometry_subset()` function. `geometry_subset` takes a geometry object (like the 'geometry' attribute of a GeoDataFrame row) and uses it as a bounding box (box-ish) for our \"Raster Dataset\", returning a NumPy array.\n",
        "\n",
        "Since `geometry_subset` takes just one geometry, we'll\n",
        "1. Dissolve all parks designated as \"Marine\" into one geometry.\n",
        "2. Use this geometry as the \"cookie cutter\" for our raster."
      ]
    },
    {
      "metadata": {
        "id": "lUviGUg71mM_"
      },
      "cell_type": "code",
      "source": [
        "# Create one geometry for ALL marine protected areas (Remember: MPAs are coded as '1')\n",
        "mpas_dissolved = conserve.dissolve(by='MARINE').loc['2']\n",
        "\n",
        "# Clip your array to the shape of MPAs (dissolved)\n",
        "mpa_array = rds.geometry_subset(mpas_dissolved.geometry)\n",
        "\n",
        "# Plot the clipped array\n",
        "imshow(mpa_array.squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h12Zw8bo1mNB"
      },
      "cell_type": "markdown",
      "source": [
        "Great! Now we have an array of bathymetry values clipped to MPA boundaries. We can use [any statistical methods](https://docs.scipy.org/doc/numpy-1.14.0/reference/index.html) available within NumPy on this array."
      ]
    },
    {
      "metadata": {
        "id": "ReS6UPC31mNB"
      },
      "cell_type": "code",
      "source": [
        "# Find the mean, min, max, and range of bathymetry values for MPAs.\n",
        "[mpa_array.mean(), mpa_array.min(), mpa_array.max(), mpa_array.ptp()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whvlEFrB1mND"
      },
      "cell_type": "markdown",
      "source": [
        "Let's tighten this up by placing these commands in this same function. **Note:** There are lots of preexisting functions for doing \"Raster Stats\" in Python and R, but they're prescriptive in the types of statistics they return. By converting your raster into an array, you can apply anything in the **NumPy** universe to your data!"
      ]
    },
    {
      "metadata": {
        "id": "q9kdCevE1mNE"
      },
      "cell_type": "code",
      "source": [
        "# Define a function encompassing all of the commands above.\n",
        "# Clip Raster Dataset to vector object g.\n",
        "# Find mean, min, max, and range values of clipped numpy array.\n",
        "# Fetch the MPA ID from \"g\" and place alongside summary statistics.\n",
        "def stats_bathy(g, rds):\n",
        "    mpa_arr = rds.geometry_subset(g.geometry)\n",
        "    mean_bathy = mpa_arr.mean()\n",
        "    min_bathy = mpa_arr.min()\n",
        "    max_bathy = mpa_arr.max()\n",
        "    range_bathy = mpa_arr.ptp()\n",
        "    mpa_id = g['WDPA_PID']\n",
        "    return mpa_id, mean_bathy, min_bathy, max_bathy, range_bathy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlfa_gQ81mNG"
      },
      "cell_type": "code",
      "source": [
        "# Try the function on g = mpas_dissolved\n",
        "stats_bathy(mpas_dissolved, rds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FJJ5bLgB1mNI"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 4.1"
      ]
    },
    {
      "metadata": {
        "id": "lZ9QyjD31mNI"
      },
      "cell_type": "code",
      "source": [
        "# Follow the steps above to clip your raster data to MPA boundaries."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Te55IqNS1mNL"
      },
      "cell_type": "code",
      "source": [
        "# Define a new function calculating the standard deviation and coefficient of variation of MPA geometries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G48ciIuA1mNM"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4.2: Apply bathymetry to many \"zones\""
      ]
    },
    {
      "metadata": {
        "id": "yVfFHRpz1mNM"
      },
      "cell_type": "markdown",
      "source": [
        "Now that our personal \"Raster Stats\" are bundled neatly in one function, we can iterate through multiple geometries, for example, each individual MPA (undissolved). The easiest way to do this is to use `apply`, which operates like `apply` in R. Our steps are:\n",
        "\n",
        "* Create an anonymous function using `lambda` and run each geometry (row) in our DataFrame through that function.\n",
        "* `apply` basically **points each geometry in our DataFrame**  to fill `g` in `stats_bathy(g, rds)`"
      ]
    },
    {
      "metadata": {
        "id": "qhATIhvN1mNN"
      },
      "cell_type": "code",
      "source": [
        "# Define a new function that uses 'lambda' to iterate stats_bathy through multiple DataFrame rows (MPA geometries).\n",
        "def stats_bathy_multiple(dataframe, rds):\n",
        "    series_mpas = dataframe.apply(lambda g: stats_bathy(g, rds), axis=1)\n",
        "    return series_mpas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pg1TH5JQ1mNP"
      },
      "cell_type": "code",
      "source": [
        "# Try out the function on our original 'mpa' GeoDataFrame\n",
        "series_mpas = stats_bathy_multiple(mpa, rds)\n",
        "series_mpas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-Eewgcc1mNQ"
      },
      "cell_type": "markdown",
      "source": [
        "We now have summary statistics for all MPAs in our `conserve` dataset!"
      ]
    },
    {
      "metadata": {
        "id": "hftFLbOL1mNQ"
      },
      "cell_type": "markdown",
      "source": [
        "## Extra steps"
      ]
    },
    {
      "metadata": {
        "id": "7vPPh_GD1mNQ"
      },
      "cell_type": "markdown",
      "source": [
        "The data above isn't in the most usable format. Our function returns a series of tuples, with each tuple representing a different MPA geometry.\n",
        "\n",
        "If I want to get all of this back into a DataFrame, I can split each tuple into separate values (attributes). There are several ways to do this, but one easy method is `vstack()`, which splits *thing* into multiple *sub-things* and stacks them vertically to make an array. I can then insert this data into a DataFrame."
      ]
    },
    {
      "metadata": {
        "id": "Cdy1gDr01mNR"
      },
      "cell_type": "code",
      "source": [
        "# Convert your series of tuples into a DataFrame using 'vstack'\n",
        "dataframe_mpas = pd.DataFrame(np.vstack(series_mpas))\n",
        "dataframe_mpas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ckax55Fx1mNS"
      },
      "cell_type": "code",
      "source": [
        "# Rename the columns of your DataFrame\n",
        "dataframe_mpas = dataframe_mpas.rename(columns = {0:'WDPA_PID', 1:'mean_bathy', 2: 'min_bathy', 3: 'max_bathy', 4: 'range_bathy'})\n",
        "dataframe_mpas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Snkb6CRB1mNU"
      },
      "cell_type": "markdown",
      "source": [
        "I can then save this data back out to a CSV. Saving tabular data is as simple as:\n",
        "* DataFrame = Pandas = `pd.read_csv()` = `pd.to_csv()`\n",
        "* GeoDataFrame = DataFrame with geometry attribute = GeoPandas = `gpd.read_file()` = `gpd.to_file()`"
      ]
    },
    {
      "metadata": {
        "id": "dThxTHuQ1mNV"
      },
      "cell_type": "code",
      "source": [
        "# Save DataFrame to CSV\n",
        "dataframe_mpas.to_csv(\"python_ecodatascience.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y5XjVgVU1mNV"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 5: Review"
      ]
    },
    {
      "metadata": {
        "id": "U9jteQ171mNX"
      },
      "cell_type": "markdown",
      "source": [
        "* **Use Pandas to manage and clean tabular data in a DataFrame**\n",
        "    * Use `loc` and `iloc` to index/subset data in a DataFrame.\n",
        "    * Remember to check `dtype`\n",
        "* **Use GeoPandas** to do the same for geodata\n",
        "* **NumPy is useful for managing data in arrays**, particularly if your data has over 2 dimensions.\n",
        "* A few tips for **iterating through DataFrames**:\n",
        "    * There are a bunch of iterative tools in Python--we used `apply` to iterate through GeoDataFrame rows (geometries) using axis=1 (we could also iterate through columns using axis=0). Some other important iterative tools that we didn't discuss are [loops](https://www.digitalocean.com/community/tutorials/how-to-construct-for-loops-in-python-3) and [list comprehensions](https://www.datacamp.com/community/tutorials/python-list-comprehension).\n",
        "* Data Structures in Python come in [many shapes and sizes](https://pandas.pydata.org/pandas-docs/stable/dsintro.html). Arrange your data (whatever it may be) in the structure of your choosing by 1) reading the filepath into the desired library like GeoPandas or, 2) using your data as an argument in a Data Constructor."
      ]
    }
  ]
}