{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/su-sumico/seminar/blob/main/OEM_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTGkOTdOpBll"
      },
      "source": [
        "# OpenEarhMap Semantinc Segmentation\n",
        "\n",
        "This demo code demonstrates training and testing of UNet-EfficientNet-B4 for the OpenEarthMap dataset (https://open-earth-map.org/). This demo code is based on the work from the \"segmentation_models.pytorch\" repository by qubvel, available at: https://github.com/qubvel/segmentation_models.pytorch. We extend our sincere appreciation to the original author for their invaluable contributions to the field of semantic segmentation and for providing this open-source implementation.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWiUctcOpBlo"
      },
      "source": [
        "### Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lkh6kW9pBlo",
        "outputId": "1f00b016-99d2-44ef-adf2-f3cc63a76678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQdFTlpypBlp",
        "outputId": "755dab4f-711e-4438-c42c-a1a5cbbeb130",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.10/dist-packages (from rasterio) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2023.5.7)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.6)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.22.4)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.4.7)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.0)\n",
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels) (0.15.2+cu118)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pretrainedmodels) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pretrainedmodels) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pretrainedmodels) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pretrainedmodels) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pretrainedmodels) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pretrainedmodels) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pretrainedmodels) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pretrainedmodels) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->pretrainedmodels) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->pretrainedmodels) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->pretrainedmodels) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pretrainedmodels) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->pretrainedmodels) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->pretrainedmodels) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->pretrainedmodels) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->pretrainedmodels) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio\n",
        "!pip install pretrainedmodels\n",
        "!pip install efficientnet_pytorch\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4elx5iipBlq"
      },
      "source": [
        "### Import\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhMDpFqZpBlr"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab/OEM_230725') # <= change path where you save code\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import source\n",
        "import segmentation_models_pytorch as smp\n",
        "import glob\n",
        "import torchvision.transforms.functional as TF\n",
        "import math\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os768d4EpBlr"
      },
      "source": [
        "### Define main parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOj5_N4apBlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfc3a93-3030-4614-ecdf-ba54ca866b6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of epochs   : 5\n",
            "Number of classes  : 9\n",
            "Batch size         : 4\n",
            "Device             : cuda\n"
          ]
        }
      ],
      "source": [
        "OEM_ROOT = \"/content/drive/My Drive/Colab/OEM_230725/data/OpenEarthMap_Demo/\"\n",
        "OEM_DATA_DIR = OEM_ROOT+'train_val/'\n",
        "TEST_DIR = OEM_ROOT+'test/'\n",
        "TRAIN_LIST = OEM_ROOT+\"train.txt\"\n",
        "VAL_LIST = OEM_ROOT+\"val.txt\"\n",
        "WEIGHT_DIR = \"/content/drive/My Drive/Colab/OEM_230725/weight\" # path to save weights\n",
        "OUT_DIR = \"/content/drive/My Drive/Colab/OEM_230725/result/\" # path to save prediction images\n",
        "os.makedirs(WEIGHT_DIR, exist_ok=True)\n",
        "test_large = OEM_ROOT+'N35.675E139.725.tif'\n",
        "\n",
        "seed = 0\n",
        "learning_rate = 0.0001\n",
        "batch_size = 4\n",
        "n_epochs = 5\n",
        "classes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "n_classes = len(classes)+1\n",
        "classes_wt = np.ones([n_classes], dtype=np.float32)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Number of epochs   :\", n_epochs)\n",
        "print(\"Number of classes  :\", n_classes)\n",
        "print(\"Batch size         :\", batch_size)\n",
        "print(\"Device             :\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCETH4-PpBls"
      },
      "source": [
        "### Prepare training and validation file lists\n",
        "\n",
        "In this demo for Google Colab, we use only two regions, i.e., Tokyo and Kyoto for training. To train with the full set, please download the OpenEarthMap dataset from https://zenodo.org/record/7223446. Note for xBD data preparation is available at https://github.com/bao18/open_earth_map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn8aGxUvpBls",
        "outputId": "82f2abed-3597-4e31-abbf-a0c0c3d40804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples      : 98\n",
            "Training samples   : 84\n",
            "Validation samples : 14\n"
          ]
        }
      ],
      "source": [
        "img_pths = [f for f in Path(OEM_DATA_DIR).rglob(\"*.tif\") if \"/labels/\" in str(f)]\n",
        "train_pths = [str(f) for f in img_pths if f.name in np.loadtxt(TRAIN_LIST, dtype=str)]\n",
        "val_pths = [str(f) for f in img_pths if f.name in np.loadtxt(VAL_LIST, dtype=str)]\n",
        "\n",
        "print(\"Total samples      :\", len(img_pths))\n",
        "print(\"Training samples   :\", len(train_pths))\n",
        "print(\"Validation samples :\", len(val_pths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-uFSOcgpBlt"
      },
      "source": [
        "### Define training and validation dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkjJGjCOpBlt"
      },
      "outputs": [],
      "source": [
        "trainset = source.dataset.Dataset(train_pths, classes=classes, size=512, train=True)\n",
        "validset = source.dataset.Dataset(val_pths, classes=classes, train=False)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(validset, batch_size=batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR9cFGaspBlt"
      },
      "source": [
        "### Setup network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTIQto7LpBlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dceb44e7-4f97-4865-c153-aa65fe099f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\n",
            "Model output name  : u-efficientnet-b4_s0_CELoss\n",
            "Number of parameters:  20304278\n"
          ]
        }
      ],
      "source": [
        "network = smp.Unet(\n",
        "    classes=n_classes,\n",
        "    activation=None,\n",
        "    encoder_weights=\"imagenet\",\n",
        "    encoder_name=\"efficientnet-b4\",\n",
        "    decoder_attention_type=\"scse\",\n",
        ")\n",
        "\n",
        "# count parameters\n",
        "params = 0\n",
        "for p in network.parameters():\n",
        "    if p.requires_grad:\n",
        "        params += p.numel()\n",
        "\n",
        "criterion = source.losses.CEWithLogitsLoss(weights=classes_wt)\n",
        "criterion_name = 'CE'\n",
        "metric = source.metrics.IoU2()\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
        "network_fout = f\"{network.name}_s{seed}_{criterion.name}\"\n",
        "OUT_DIR += network_fout # path to save prediction images\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Model output name  :\", network_fout)\n",
        "print(\"Number of parameters: \", params)\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Number of GPUs :\", torch.cuda.device_count())\n",
        "    network = torch.nn.DataParallel(network)\n",
        "    optimizer = torch.optim.Adam(\n",
        "        [dict(params=network.module.parameters(), lr=learning_rate)]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization functions"
      ],
      "metadata": {
        "id": "f2lQx068bQwo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTJFfnmspBlu"
      },
      "outputs": [],
      "source": [
        "class_rgb = {\n",
        "    \"Bareland\": [128, 0, 0],\n",
        "    \"Grass\": [0, 255, 36],\n",
        "    \"Pavement\": [148, 148, 148],\n",
        "    \"Road\": [255, 255, 255],\n",
        "    \"Tree\": [34, 97, 38],\n",
        "    \"Water\": [0, 69, 255],\n",
        "    \"Cropland\": [75, 181, 73],\n",
        "    \"buildings\": [222, 31, 7],\n",
        "}\n",
        "\n",
        "class_gray = {\n",
        "    \"Bareland\": 1,\n",
        "    \"Grass\": 2,\n",
        "    \"Pavement\": 3,\n",
        "    \"Road\": 4,\n",
        "    \"Tree\": 5,\n",
        "    \"Water\": 6,\n",
        "    \"Cropland\": 7,\n",
        "    \"buildings\": 8,\n",
        "}\n",
        "\n",
        "def label2rgb(a):\n",
        "    \"\"\"\n",
        "    a: labels (HxW)\n",
        "    \"\"\"\n",
        "    out = np.zeros(shape=a.shape + (3,), dtype=\"uint8\")\n",
        "    for k, v in class_gray.items():\n",
        "        out[a == v, 0] = class_rgb[k][0]\n",
        "        out[a == v, 1] = class_rgb[k][1]\n",
        "        out[a == v, 2] = class_rgb[k][2]\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "hn9_yGisbYAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t5R7gDKpBlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a0789b0-3d62-47fa-953d-1b700d006045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 21/21 [01:34<00:00,  4.50s/it, CELoss=2.34, mIoU=0.0501]\n",
            "Valid: 100%|██████████| 4/4 [00:15<00:00,  3.94s/it, CELoss=2.09, mIoU=0.0678]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved!\n",
            "\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, CELoss=2.11, mIoU=0.0986]\n",
            "Valid: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it, CELoss=2.08, mIoU=0.081]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved!\n",
            "\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, CELoss=1.97, mIoU=0.132]\n",
            "Valid: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it, CELoss=2.06, mIoU=0.0798]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 21/21 [00:16<00:00,  1.31it/s, CELoss=1.85, mIoU=0.16]\n",
            "Valid: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it, CELoss=2.08, mIoU=0.0661]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|██████████| 21/21 [00:16<00:00,  1.30it/s, CELoss=1.67, mIoU=0.195]\n",
            "Valid: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it, CELoss=2.04, mIoU=0.0873]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved!\n",
            "Processing time: 193.70987725257874\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "max_score = 0\n",
        "train_hist = []\n",
        "valid_hist = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  print(f\"\\nEpoch: {epoch + 1}\")\n",
        "\n",
        "  logs_train = source.runner.train_epoch(\n",
        "      model=network,\n",
        "      optimizer=optimizer,\n",
        "      criterion=criterion,\n",
        "      metric=metric,\n",
        "      dataloader=train_loader,\n",
        "      device=device,\n",
        "  )\n",
        "\n",
        "  logs_valid = source.runner.valid_epoch(\n",
        "      model=network,\n",
        "      criterion=criterion,\n",
        "      metric=metric,\n",
        "      dataloader=valid_loader,\n",
        "      device=device,\n",
        "  )\n",
        "\n",
        "  train_hist.append(logs_train)\n",
        "  valid_hist.append(logs_valid)\n",
        "\n",
        "  score = logs_valid[metric.name]\n",
        "\n",
        "  if max_score < score:\n",
        "      max_score = score\n",
        "      torch.save(network.state_dict(), os.path.join(WEIGHT_DIR, f\"{network_fout}.pth\"))\n",
        "      print(\"Model saved!\")\n",
        "\n",
        "end = time.time()\n",
        "print('Processing time:',end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n"
      ],
      "metadata": {
        "id": "ONWiNQMWAHP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "# load network\n",
        "network.load_state_dict(torch.load(os.path.join(WEIGHT_DIR, f\"{network_fout}_pretrained.pth\")))\n",
        "network.to(device).eval()\n",
        "\n",
        "test_pths = glob.glob(TEST_DIR+\"/*.tif\")\n",
        "#testset = source.dataset.Dataset(test_pths, classes=classes, train=False)\n",
        "\n",
        "for fn_img in test_pths:\n",
        "  img = source.dataset.load_multiband(fn_img)\n",
        "  h, w = img.shape[:2]\n",
        "  power = math.ceil(np.log2(h) / np.log2(2))\n",
        "  shape = (2 ** power, 2 ** power)\n",
        "  img = cv2.resize(img, shape)\n",
        "\n",
        "  # test time augmentation\n",
        "  imgs = []\n",
        "  imgs.append(img.copy())\n",
        "  imgs.append(img[:, ::-1, :].copy())\n",
        "  imgs.append(img[::-1, :, :].copy())\n",
        "  imgs.append(img[::-1, ::-1, :].copy())\n",
        "\n",
        "  input = torch.cat([TF.to_tensor(x).unsqueeze(0) for x in imgs], dim=0).float().to(device)\n",
        "\n",
        "  pred = []\n",
        "  with torch.no_grad():\n",
        "      msk = network(input)\n",
        "      msk = torch.softmax(msk[:, :, ...], dim=1)\n",
        "      msk = msk.cpu().numpy()\n",
        "      pred = (msk[0, :, :, :] + msk[1, :, :, ::-1] + msk[2, :, ::-1, :] + msk[3, :, ::-1, ::-1])/4\n",
        "  pred = pred.argmax(axis=0).astype(\"uint8\")\n",
        "  size = pred.shape[0:]\n",
        "  y_pr = cv2.resize(pred, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "  # save image as png\n",
        "  filename = os.path.splitext(os.path.basename(fn_img))[0]\n",
        "  y_pr_rgb = label2rgb(y_pr)\n",
        "  Image.fromarray(y_pr_rgb).save(os.path.join(OUT_DIR, filename+'_pr.png'))\n",
        "\n",
        "end = time.time()\n",
        "print('Processing time:',end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0slbVQcmQlp",
        "outputId": "133da56c-c059-4abf-b290-8b5401ab62ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing time: 9.143620014190674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing a model for a large Geotiff image\n",
        "\n",
        "A sample image is provided by the Geospatial Information Authority of Japan at https://cyberjapandata.gsi.go.jp/xyz/seamlessphoto/{z}/{x}/{y}.jpg\n"
      ],
      "metadata": {
        "id": "E3I0hKRyHMl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "# load network\n",
        "network.load_state_dict(torch.load(os.path.join(WEIGHT_DIR, f\"{network_fout}_pretrained.pth\")))\n",
        "network.to(device).eval()\n",
        "\n",
        "# process large Geotiff image\n",
        "img0 = source.dataset.load_multiband(test_large)\n",
        "\n",
        "# get crs and transform\n",
        "crs, trans = source.dataset.get_crs(test_large)\n",
        "\n",
        "if img0.shape[2] > 3:\n",
        "    img0 = img0[:,:,[0,1,2]]\n",
        "height = img0.shape[0]\n",
        "width = img0.shape[1]\n",
        "band = img0.shape[2]\n",
        "\n",
        "patch_size = 512\n",
        "stride = 256\n",
        "C = int(np.ceil( (width - patch_size) / stride ) + 1)\n",
        "R = int(np.ceil( (height - patch_size) / stride ) + 1)\n",
        "\n",
        "# weight matrix B for avoiding boundaries of patches\n",
        "if patch_size > stride:\n",
        "    w = patch_size\n",
        "    s1 = stride\n",
        "    s2 = w - s1\n",
        "    d = 1/(1+s2)\n",
        "    B1 = np.ones((w,w))\n",
        "    B1[:,s1::] = np.dot(np.ones((w,1)),(-np.arange(1,s2+1)*d+1).reshape(1,s2))\n",
        "    B2 = np.flip(B1)\n",
        "    B3 = B1.T\n",
        "    B4 = np.flip(B3)\n",
        "    B = B1*B2*B3*B4\n",
        "else:\n",
        "    B = np.ones((w,w))\n",
        "\n",
        "img1 = np.zeros((patch_size+stride*(R-1), patch_size+stride*(C-1),3))\n",
        "img1[0:height,0:width,:] = img0.copy()\n",
        "\n",
        "pred_all = np.zeros((9,patch_size+stride*(R-1), patch_size+stride*(C-1)))\n",
        "weight = np.zeros((patch_size+stride*(R-1), patch_size+stride*(C-1)))\n",
        "\n",
        "for r in range(R):\n",
        "    for c in range(C):\n",
        "        img = img1[r*stride:r*stride+patch_size,c*stride:c*stride+patch_size,:].copy().astype(np.float32)/255\n",
        "        imgs = []\n",
        "        imgs.append(img.copy())\n",
        "        imgs.append(img[:, ::-1, :].copy())\n",
        "        imgs.append(img[::-1, :, :].copy())\n",
        "        imgs.append(img[::-1, ::-1, :].copy())\n",
        "\n",
        "        input = torch.cat([TF.to_tensor(x).unsqueeze(0) for x in imgs], dim=0).float().to(device)\n",
        "\n",
        "        pred = []\n",
        "        with torch.no_grad():\n",
        "            msk = network(input)\n",
        "            msk = torch.softmax(msk[:, :, ...], dim=1)\n",
        "            msk = msk.cpu().numpy()\n",
        "\n",
        "            pred = (msk[0, :, :, :] + msk[1, :, :, ::-1] + msk[2, :, ::-1, :] + msk[3, :, ::-1, ::-1])/4\n",
        "\n",
        "        pred_all[:,r*stride:r*stride+patch_size,c*stride:c*stride+patch_size] += pred.copy()*B\n",
        "        weight[r*stride:r*stride+patch_size,c*stride:c*stride+patch_size] += B\n",
        "\n",
        "for b in range(9):\n",
        "    pred_all[b,:,:] = pred_all[b,:,:]/weight\n",
        "    if b == 0:\n",
        "        pred_all[b,:,:] = 0\n",
        "\n",
        "pred_all = pred_all.argmax(axis=0).astype(\"uint8\")\n",
        "\n",
        "filename = os.path.splitext(os.path.basename(test_large))[0]\n",
        "pr_rgb = label2rgb(pred_all)\n",
        "Image.fromarray(pr_rgb[0:height,0:width,:]).save(os.path.join(OUT_DIR, filename+'_pr.png'))\n",
        "\n",
        "# save geotiff\n",
        "pr_rgb = np.transpose(pr_rgb[0:height,0:width,:], (2,0,1))\n",
        "source.dataset.save_img(os.path.join(OUT_DIR, filename+'_pr.tif'),pr_rgb,crs,trans)\n",
        "\n",
        "end = time.time()\n",
        "print('Processing time:',end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WguTdAFqHWAe",
        "outputId": "f33e8f67-8953-4fca-bc11-11b670003234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing time: 88.54579997062683\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('pcc')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8adc929056e836a7a3b0e312c82a4cd60bdf00e8c79486eca7d6ecc652c519a"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}